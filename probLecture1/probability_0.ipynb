{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gqp7CBGVCX1x"
   },
   "source": [
    "topics: \n",
    "\n",
    "---\n",
    "\n",
    "* frequency probability (YOEL)\n",
    "* subjective probability (YOEL)\n",
    "* random variables (RACHEL)\n",
    "* continuous vs discrete R.V.s (RACHEL) \n",
    "* conditional probability (YOEL) \n",
    "* marginal probability (YOEL)\n",
    "* joint probability (RACHEL)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIbA9ARdCUQR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4QyHDcKSDANY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qena_yEsEQyF"
   },
   "source": [
    "## Assigning numbers to groups of things\n",
    "\n",
    "In very loose terms the framework of probability theory tells you how you should assign numbers, in a very specific ways, to groups of things.\n",
    "\n",
    "This might be a bit slow but we'll try and build up in a reasonable amount of time. Imagine you have 3 groups, and inside these groups you have numbers between 1 and 10. Each group has some subset of the numbers between 1 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cUCJaGdeESDW",
    "outputId": "2550a168-8b52-4b0b-c106-d4420a685d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupA = :  [ 1  1  1  3  3  4  5  6  7  7  7  8  9 10]\n",
      "groupB = :  [2 4 6 8]\n",
      "groupC = :  [ 1  3  5  7  9 10]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "groupA = np.array([1, 1, 1, 3, 3, 4, 5, 6, 7, 7, 7, 8, 9, 10])\n",
    "print(\"groupA = : \", groupA)\n",
    "\n",
    "groupB = np.arange(2, 10, 2)\n",
    "print(\"groupB = : \", groupB)\n",
    "\n",
    "groupC = np.setdiff1d(groupA, groupB)\n",
    "print(\"groupC = : \", groupC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Zy1QnkIEjAe"
   },
   "source": [
    "If you read books on probability theory the above might have the following names attached to them: \n",
    "\n",
    "groupA: **sample space**\n",
    "\n",
    "groupB: **event, a subset of the sample space**\n",
    "\n",
    "3: **outcome of the event groupC**\n",
    "\n",
    "it's not too important that you memorize these definitions but we just want to make you aware of them. Now that we have \"groups of things\" we want to use the rules of probability theory to consistently assign numbers to \"things happening\". Usually when someone asks about a probability of an event it roughly falls into an interpertation in terms of: \n",
    "\n",
    "## frequency of event happening\n",
    "\n",
    "imagine you sample a number randomly from groupA, and you tally up the times that the number(s) comes up then divide by the total amount of times sampled. If we didn't know all of the possible outcomes of groupA then we could define the probability of those numbers as so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "0os81VcqEaDM",
    "outputId": "7395a80e-3d79-47d8-e30d-56efd2625a87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key, probability:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.214932),\n",
       " (3, 0.14412),\n",
       " (4, 0.071102),\n",
       " (5, 0.071496),\n",
       " (6, 0.071382),\n",
       " (7, 0.213216),\n",
       " (8, 0.07143),\n",
       " (9, 0.071248),\n",
       " (10, 0.071074)]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nSamples = 500000\n",
    "samples = np.random.choice(groupA, nSamples)\n",
    "sampleDist = [(key, val / nSamples) for key, val in sorted(Counter(samples).items())]\n",
    "print(\"key, probability:\")\n",
    "sampleDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EGwg7qfExhU"
   },
   "source": [
    "it happens to be the case that because we do know the entire sample space we can analyically compute the probability of each number, which we do below _(we can almost never do this \"in the real world\" with real data)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "pC8FaoBiEnvh",
    "outputId": "4116dfe1-ad58-461a-88bc-932ac5b691c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key, probability:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.21428571428571427),\n",
       " (3, 0.14285714285714285),\n",
       " (4, 0.07142857142857142),\n",
       " (5, 0.07142857142857142),\n",
       " (6, 0.07142857142857142),\n",
       " (7, 0.21428571428571427),\n",
       " (8, 0.07142857142857142),\n",
       " (9, 0.07142857142857142),\n",
       " (10, 0.07142857142857142)]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trueN = len(groupA)\n",
    "print(\"key, probability:\")\n",
    "trueDist = [(key, val / trueN) for key, val in sorted(Counter(groupA).items())]\n",
    "trueDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnAAgw11E1Zi"
   },
   "source": [
    "The above is called usually called the frequency view of probability, but it isn't the only way we can decide to assign probability. For example, there are certain events that only happen once, and so in this case, running a simulation to try and get at the underlying probability of events is impossible. \n",
    "\n",
    "## Degree of belief\n",
    "\n",
    "We could alternatively imagine a case where an event only happens once, as in a presidential election. In this case we might say that we are reasonaly sure our favorite candidate will win, and assign a number to that degree of belief. This is called a subjective probability. \n",
    "\n",
    "In any case both definitions of probability rely on the concept of **random variables**.\n",
    "\n",
    "\n",
    "## Formal definition: \n",
    "\n",
    "A _probability space_ contains two elements, a sample space $G$ and a probability function $P(\\cdot)$ which takes an event $A$ (that is a subset of $G$) as input and returns a real number between 0 and 1: $P(A)$\n",
    "\n",
    "There are particular properties that together a sample space and probability function must meet, these properties further rely on 2 basic claims: \n",
    "\n",
    "1. non-negativity: $P(nothing) = 0$ and $P(G) = 1$\n",
    "2. discrtization: $P(\\bigcup_{A_i \\in G} A_i) = \\sum _{A_i \\in G} P(A_i)$\n",
    "\n",
    "in words, the above states that disjoint events are mutually exclusive: e.g. $P(A_i , A_j) = 0$ if $i \\neq j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqcJ_iFrE0HQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kkNeXt__KBOH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-mE5AOjKBdk"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fIsOP_VzJ_-K"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYzEc8fEKAZR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irdRYaEEKAry"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x59AteTqG22a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPv0Gy9QG68f"
   },
   "source": [
    "# Random Variables\n",
    "\n",
    "A random variable is just like a normal variable we use in algebra, except it's value depends on the outcome of a random phenomenon. Random variables are an important part of Probability Theory. \n",
    "\n",
    "For example, we can consider random variable $X$ that denotes the outcome of a coin flip. If the coin flips to heads, $X = 0$, to tails, $X = 1$. We can consider different properties of random variable X such as how likely $X = 0$ or $X = 1$. We use the random variable because we don't quite know the possible outcome, and we want to be able to measure the probabilities for each outcome (altogether forming a probaiblity distribution). \n",
    "\n",
    "\n",
    "\n",
    "## Discrete Random Variables\n",
    "\n",
    "The coin flip described earlier is an example of a *discrete* random variable because the possible values for X is finite or countably infinite. \n",
    "\n",
    "We can consider functions on discrete variables such as the **probability mass function** of a random variable which describes how likely a random variable $X$ can take on some value $x$. We can formally define this for disecrete random variables as $p(x) = P(X = x)$ such that $x$ denotes each possible value that $X$ can take. \n",
    "\n",
    "In the coin flip case, we can say that the set of possible values for $X$ is $X = {0, 1}$, a finite set. If the coin is fair, then we can say that $p(0) = p(X = 0) = 0.5$ and $p(1) = p(X = 1) = 0.5$. \n",
    "\n",
    "You can see that it will always be true that $\\sum_{x \\in X} p(x) = 1$ or that the sum of the probability of X being all its possible values will be 1. \n",
    "\n",
    "\n",
    "We can also consider the **cumulative distribution function (cdf)** of a random variable which describes how likely a random variable $X$ can be less than or equal to some value $x$. This is formally defined as $F(x) = P(X <= x)$ and specifically for discrete variables as $F(x) = \\sum_{t\\in X: t x} p(t)$. \n",
    "\n",
    "Examples of discrete random variables include Bernoulli (eg. the coin flip, probability $p$ for one outcome, $1-p$ for the other outcome), Binomial (a series of Bernoulli's), Poisson, discrete uniform variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11VSxM6UMc2h"
   },
   "source": [
    "**EXAMPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HLBWrcMMXsq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "probability-0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
