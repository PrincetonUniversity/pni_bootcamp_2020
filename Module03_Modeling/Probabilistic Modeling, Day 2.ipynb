{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write a function that, given a series of coin flips, will return the maximum likelihood estimate of the coin's bias. Accomplish this by trying all possible biases between 0 and 1 in steps of 0.05, and returning the bias that yields the highest log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write a function that, given a series of samples from a normal distribution with unknown mean $\\mu$ and known standard deviation $\\sigma = 10$, will return the maximum likelihood estimate for $\\mu$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Write a function that, given a series of samples from a normal distribution with unknown mean $\\mu$ and known standard deviation $\\sigma = 10$, will return the maximum a posteriori estimate for $\\mu$. You may assume a prior belief for $\\mu$ that is $\\mu \\sim N(3, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Write a function that, given a series of samples from a normal distribution with unknown mean $\\mu$ and unknown standard deviation $\\sigma$, will return the maximum likelihood estimates for both $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cooler stuff\n",
    "\n",
    "In this section of the exercises, you're going to write inference code for the models you coded yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Neural coding (Poisson-spiking neurons)\n",
    "\n",
    "In this example, we again consider a fictional neuronal subtype that fires according to the azimuthal (horizontal) angle of the eye in goldfish. This neuron has a firing rate function\n",
    "\n",
    "$$\n",
    "    f(x) = kx\n",
    "$$\n",
    "\n",
    "where $x \\in [0, 90]$ corresponds to the eye angle, $k$ is some positive constant, and $f(x)$ denotes the firing rate of the neuron at angle $x$ in Hz. It then emits a spike count $r$ according to a Poisson distribution with rate parameter $f(x)$. That is, the distribution of possible spike counts $r$ at location $x$ is given by:\n",
    "\n",
    "$$\n",
    "P(r | x) = Poisson(f(x))\n",
    "$$\n",
    "\n",
    "In this exercise, we will attempt to infer the value of $k$. You may either use your own generative code from yesterday to create a dataset, or download the dataset we've generated for you off of the course GitHub page. \n",
    "\n",
    "First, write a function that accepts a vector $r$, a vector $x$, and a real number $k$. This function should return the log-likelihood of the data (i.e., the log-probability of observing spike count $r[i]$ at eye angle $x[i]$ for all $i$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement grid search to infer $k$. As noted in lecture, grid search is simply the process of dividing up the regions of \"plausible\" values of $k$ into a fine grid, systematically trying each possible value, and then returning the best one.\n",
    "\n",
    "To that end, write a function that loops over possible values of $k$ from $-30$ to $30$ in small steps (say, $0.1$). Let's denote the current guess value $\\hat{k}$. For each $\\hat{k}$ between $-30$ and $30$, compute the log-likelihood of the data with $k = \\hat{k}$. At the end of the loop, you should return the $\\hat{k}$ that had the highest log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your inferred $k$ value to plot $f(x)$ for our fictional neuron, and overlay a scatterplot of the true data. How good does the fit look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge: Download the second dataset on the course GitHub, and run your fitting and plotting functions on that dataset. What do you notice? Can you think of a way to alter the generative model to account for what you see? Edit your code above and see if you can figure out a way to fit both datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Psychometric curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we consider the behaviour of a mouse performing an evidence accumulation task. The mouse hears a stream of randomly-generated clicks on both the right and left sides of its head. After 60 seconds pass, the mouse must pull a lever corresponding to the side that had the most clicks. \n",
    "\n",
    "Today, we will perform inference on this model.\n",
    "\n",
    "As above, write a function that accepts a vector of choices, a $1 \\times T$ vector $\\vec{x}$ of click deltas (i.e., $x[i]$ is the difference $n_L - n_R$ on trial $i$), as well as the logistic function parameters $k, x_0$. This function should return the log-likelihood of the choices made by the mouse given the click deltas $\\vec{x}$ and the parameters $k, x_0$. \n",
    "\n",
    "For reference, the logistic function is: \n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-k(x - x_0)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we're going to be using scipy's `minimize` function, which finds the minimum of a function using *fancy* methods. How should you adjust the code you've written above to do this? Write a function that will call `minimize` to return estimates for both $k$ and $x_0$. (This will likely be challenging, but give it a try first.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your inferred parameters to plot $f(x)$ for our mouse, and overlay a scatterplot of the true data. How good does the fit look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this mouse exhibit a lateral bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Write answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in lecture, optimization is about finding parameters that maximize or minimize some objective function. To really visualize this idea, plot the log-likelihood landscape as a 3D surface as a function of $k, x_0$. Where do your fitted parameters end up relative to this landscape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
