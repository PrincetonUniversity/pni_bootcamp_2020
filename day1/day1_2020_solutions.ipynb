{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 Exercises (NumPy + Matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic NumPy Operations\n",
    "a) Generate an array of numbers 0-24. Reshape to a 5x5 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # We'll do this once for the whole littany of exercises\n",
    "\n",
    "mat = np.arange(25).reshape(5,5)\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Extract the diagonal of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  6 12 18 24]\n"
     ]
    }
   ],
   "source": [
    "diag = np.diag(mat)\n",
    "print(diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Multiply the matrix by an identity matrix of the same shape. Confirm that it is identical to the original.\n",
    "\n",
    "Hint: Use `np.all` command to confirm all equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## Construct identity matrix.\n",
    "idmat = np.identity(5)\n",
    "## Note that we could also have passed mat.shape[0] instead of 5. Then our code would be agnostic about the shape of\n",
    "## mat (other than its being square) and would work as-is even if we had done a 6x6 or 8x8 example or whatever.\n",
    "\n",
    "## Matrix multiplication.\n",
    "mat2 = mat @ idmat\n",
    "\n",
    "## Confirm all equal.\n",
    "print( np.all(mat == mat2) )\n",
    "\n",
    "## As a one-liner, if we didn't need to hold onto any of the intermediate values, we could also have done:\n",
    "## np.all( mat == (mat @ np.identity(mat.shape[0])) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Join the matrix with itself and return a new matrix with shape (2,5,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5)\n",
      "[[[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]\n",
      "  [10 11 12 13 14]\n",
      "  [15 16 17 18 19]\n",
      "  [20 21 22 23 24]]\n",
      "\n",
      " [[ 0  1  2  3  4]\n",
      "  [ 5  6  7  8  9]\n",
      "  [10 11 12 13 14]\n",
      "  [15 16 17 18 19]\n",
      "  [20 21 22 23 24]]]\n"
     ]
    }
   ],
   "source": [
    "## Here's one simple way to do this\n",
    "mat3 = np.array([mat, mat])\n",
    "print(mat3.shape)\n",
    "print(mat3)\n",
    "\n",
    "## Note that np.vstack will NOT work (that glues along axis zero, to make a 10x5 array)\n",
    "\n",
    "## However, np.concatenate along with prepending a length-1 axis to mat *will* work\n",
    "## (but it's more complex and less readable)\n",
    "## print(np.concatenate( (mat[np.newaxis,...],mat[np.newaxis,...]) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Compute the mean of the concatenated matrix along the first axis. Confirm its equal to the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Take mean.\n",
    "avg_of_both_mats_in_mat3 = mat3.mean(axis=0)\n",
    "\n",
    "## Taking the mean along axis0 means you traverse along that axis.  So you'd be taking 25 means, i.e. the mean of\n",
    "## 0 & 0, and the mean of 1 & 1, 2 & 2, etc to fill the 5x5 slots.  But that would just give us a copy of mat, right?\n",
    "print(avg_of_both_mats_in_mat3)\n",
    "\n",
    "## Confirm all equal.\n",
    "print( np.all(mat == avg_of_both_mats_in_mat3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Return the indices of the matrix where the elements are greater than 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([3, 3, 3, 3, 4, 4, 4, 4, 4]), array([1, 2, 3, 4, 0, 1, 2, 3, 4]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(3, 1), (3, 2), (3, 3), (3, 4), (4, 0), (4, 1), (4, 2), (4, 3), (4, 4)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is precisely what np.where() does if you don't give the optional [x y] arguments listed in the docstring\n",
    "print(np.where(mat > 15))\n",
    "# That's all the axis0 indices and the corresponding axis1 indices of the pertinent elements.\n",
    "\n",
    "# If you want these indices formatted as (axis0,axis1) pairs, we can use Python's zip function\n",
    "# along with Python's * operator (to unpack the pair of arrays, since zip wants each array as a separate argument)\n",
    "list(zip(*np.where(mat > 15)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Using `np.where`, set all elements of the matrix greater than 15 to 1, else 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 1 1 1]\n",
      " [1 1 1 1 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 1 1 1]\n",
      " [1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# For this, use the other syntax of np.where(), which *will* return a (brand new) array meeting the specified conditions\n",
    "print(np.where(mat > 15, 1, 0))\n",
    "# Note that \"set\" here is a bit of a misnomer, since fancy indexing using np.where() will trigger a copy\n",
    "\n",
    "# Fun alternate solution w/o where():\n",
    "# Make the boolean matrix (mat > 15) -- remember, comparisons are vectorized! -- and reinterpret\n",
    "# False as 0 and True as 1 by viewing to int8 (bools occupy one byte, so to change the view rather than make a copy,\n",
    "# we should review as a byte-sized int)\n",
    "print((mat > 15).view(dtype=np.int8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Set all elements of the matrix greater than 15 to 2, less than 5 to 1, else 0.\n",
    "\n",
    "Hint: `np.where` can be passed as an input to `np.where`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [1, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(mat > 15, 2, np.where(mat > 5, 1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Return the lower triangle of the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0]\n",
      " [ 5  6  0  0  0]\n",
      " [10 11 12  0  0]\n",
      " [15 16 17 18  0]\n",
      " [20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "# This requires reading about the \"linear-algebra-friendly-functions\" part of the notebook\n",
    "print(np.tril(mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Define a demean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean(arr):\n",
    "    \"\"\"De-mean array.\"\"\"\n",
    "    return arr - np.mean(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k) Apply the demean function across each row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]]\n",
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n",
      "[[ 2.]\n",
      " [ 7.]\n",
      " [12.]\n",
      " [17.]\n",
      " [22.]]\n",
      "[[-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# You can use np.apply_along_axis to move along axis1\n",
    "print(np.apply_along_axis(demean, 1, mat))\n",
    "\n",
    "# But note that the \"return\" line of demean() would come close to accomplishing this if we put 'mat' in place of 'arr',\n",
    "# thanks to broadcasting rules.  So using the demean() function at all is unnecessary if take means along axis1 and\n",
    "# have it keep the axis0 dimension...\n",
    "print(mat)\n",
    "print(np.mean(mat, axis=1, keepdims=True))\n",
    "print(mat - np.mean(mat, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Spike Detection\n",
    "\n",
    "In the following exercises, you will be manipulating, analyzing, and visualizing preprocessed extracellular electrophysiological data. Specifically, the following 10s recording was taken from the abdomen of a crayfish. Action potentials are readily apparent throughout the entire recording. \n",
    "\n",
    "First, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Load data.\n",
    "npz = np.load('spikes.npz')\n",
    "data = npz['data'] * 1e6      # Convert to uV\n",
    "times = npz['times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the entire raw recording. Do multiple types of spikes appear to be present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In a recent paper, [Rey et al. (2015)](https://www.sciencedirect.com/science/article/pii/S0361923015000684) suggest a simple spike detection technique via data-driven amplitude thresholding. Specifically, they propose an automated amplitude threshold that defined as multiple of an estimate of the standard deviation of the noise:\n",
    "\n",
    "$$ \\text{threshold} = k \\cdot \\hat{\\sigma}_n $$\n",
    "\n",
    "where $k$ is a constant typically between 3-5; and $\\hat{\\sigma}_n$ is an estimate of the standard deviation of the noise, defined as:\n",
    "\n",
    "$$ \\hat{\\sigma}_n = \\frac{\\text{median} \\left( |X| \\right)}{0.6745} $$ \n",
    "\n",
    "where $|X|$ is the absolute value of the raw data.\n",
    "\n",
    "Write a function that returns the amplitude threshold as defined above. The function should accept as arguments the raw data, $X$, and the constant, $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Next we need a function that can detect slices of the raw signal that exceed the threshold. This ultimately becomes a clustering problem (i.e. identifying \"islands\" of signal rising above an \"ocean of noise\"). Though this is definitely doable with core NumPy, the SciPy library has built-in functions specifically written for these purposes. \n",
    "\n",
    "Because these functions are beyond the scope of the bootcamp, we have provided a peak finding function for you. The function, `peak_finder`, accepts a raw data trace and a threshold. It then finds all clusters of samples above a threshold, and returns the index and signal magnitude corresponding to the peak of each cluster.\n",
    "\n",
    "The function relies on the `measurements` tools from scipy.ndimage. For a tutorial, see [here](https://dragly.org/2013/03/25/working-with-percolation-clusters-in-python/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finder(X, thresh):\n",
    "    \"\"\"Simple peak finding algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like, shape (n_times,)\n",
    "        Raw data trace.\n",
    "    thresh : float\n",
    "        Amplitude threshold.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    peak_loc : array_like, shape (n_clusters,)\n",
    "        Index of peak amplitudes.\n",
    "    peak_mag : array_like, shape (n_clusters,)\n",
    "        Magnitude of peak amplitudes.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.ndimage import measurements\n",
    "    \n",
    "    ## Error-catching.\n",
    "    assert X.ndim == 1\n",
    "    \n",
    "    ## Identify clusters.\n",
    "    clusters, ix = measurements.label(X > thresh)\n",
    "    \n",
    "    ## Identify index of peak amplitudes. \n",
    "    peak_loc = np.concatenate(measurements.maximum_position(X, labels=clusters, index=np.arange(ix)+1))\n",
    "    \n",
    "    ## Identify magnitude of peak amplitudes.\n",
    "    peak_mag = measurements.maximum(X, labels=clusters, index=np.arange(ix)+1)\n",
    "    return peak_loc, peak_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Apply the peak detection algorithm to the raw data using a constant $k=6$. Plot a histogram of the spike amplitudes (try bins of 0-150 in increments of 5 uV). \n",
    "\n",
    "How many spikes are detected? How many types of spikes do there appear to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the first second of the data. Using a scatterplot (or any other method you can think of), indicate the peak for each detected spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Remake the plot above, but repeating the procedure with a constant $k=2$. How trustworthy is the spike detection algorithm with this more liberal threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Returning now to the detected spikes when $k=6$, define a set of boundaries that divides the spikes into three clusters. How many spikes are in each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Action potentials last roughly 1-2 milliseconds. With this in mind, extract a 3 ms window around each detected spike; that is, extract 1.5 ms of samples on either side of the detected peak. Store each epoch according to its cluster. \n",
    "\n",
    "Hint: The data were recorded at 10 KHz meaning there are 10 samples per millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Plot each averaged spike waveform in a single plot. Add a legend denoting the spike cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB images (from MIT Lincoln Labs)\n",
    "\n",
    "A digital image is simply an array of numbers, which instructs a grid of pixels on a monitor to shine light of specific colors, according to the numerical values in that array.\n",
    "\n",
    "An RGB-image can thus be stored as a 3D NumPy array of shape-(V,H,3). V is the number of pixels along the vertical direction, H is the number of pixels along the horizontal, and the size-3 dimension stores the red, blue, and green color values for a given pixel. Thus a (32,32,3)\n",
    "\n",
    "array would be a 32x32 RGB image.\n",
    "\n",
    "You often work with a collection of images. Suppose we want to store N images in a single array; thus we now consider a 4D shape-(N, V, H, 3) array. For the sake of convenience, letâ€™s simply generate a 4D-array of random numbers as a placeholder for real image data.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* generate a 4D array that holds 500, 48x48 random RGB images (think about the shape this array should have, and use np.random.rand liberally)\n",
    "\n",
    "* then, normalize those images (by dividing through by max intensity) so that the largest intensity within each color channel within each image is set to 1, but relative intensities are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on the array shape\n",
    "\n",
    "Focus for a moment on a single RGB image (which will be a 3D array). There's a question (with no cut-and-dry correct answer) about which of two shapes feels more natural for this structure.  Let's compare two candidates (48,48,3) vs (3,48,48)  (I think we can all agree that (48,3,48) is unnatural and weird for a host of reasons, but chime in if you think we're mistaken about that).\n",
    "\n",
    "**(48,48,3)** -- this shape lends itself to thinking about a single 48x48 object (the image as you'd probably view it, namely as a 48x48 square of pixels), with a trio of numbers (the R, G, and B intensities) stored on each pixel.  This is also the shape that the exercise suggested.  But you'd be perfectly within your rights to instead opt for...\n",
    "\n",
    "**(3,48,48)** -- this shape instead lends itself to conceiving of this as 3 \"channels\" at top level (i.e. axis0 level) -- a Red channel, a Green, and a Blue -- and that associated with each channel is a 48x48 image.  This view might feel more natural if you want to think in terms of \"color filters\", and thinking of each image as really being 3 separate images (one Red, one Green, one Blue) that you'd overlay to make the composite image.\n",
    "\n",
    "So which view is \"correct\"?  Or \"better\"?  Again, no right answer to this.  It really depends on how you want to conceptualize the composite image and on which shape will make slicing easier on your cognitively given the tasks you want to perform with these images.  The answers show things both ways.\n",
    "\n",
    "Either way, it's probably most natural to have the N=500 axis *pre*pended to that shape -- so (500,48,48,3) or (500,3,48,48).  That makes it natural to conceptualize this (again, if we tend to think in axis0-first order) as 500 versions of whichever of the two shapes above you opted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (48,48,3)-based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Generate a collection of 500 48x48 RGB images\n",
    "images = np.random.rand(500, 48, 48, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the max-intensity within each color-channel of each image...\n",
    "\n",
    "# The `axis` option tells max() to treat all the data along axes 1 & 2 (for fixed indices on axes 0 & 3)\n",
    "# as though those 48x48 values were all a single 1D chain, and to take the max among them. The net effect\n",
    "# is to replace axes 1 & 2 (again, for each fixed pair of indices on axes 0 & 3) with a scalar, namely the\n",
    "# max value.  The result will be a 500x3 array holding the maxR, maxG, and maxB values for each of the 500 images.\n",
    "max_RGB = images.max(axis=(1,2))  \n",
    "max_RGB.shape  # No need for print() -- notebook will display the last variable referenced by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to broadcast-divide all the R,G,B values on each by their respective maxes in one vectorized swoop.\n",
    "# But (500,3) isn't compatible with (500,48,48,3) (make sure you understand *why* based on broadcasting rules).\n",
    "# But we can make them compatible by manually inserting size-1 dimensions so that the maxes take on shape\n",
    "# (500, 1, 1, 3).  And that allows us to divide in a vectorize numpythonic way.  Note: no need to have a new variable\n",
    "# point to the reshaped array, since once the division is over, we don't need to hold on to the reshaped version.\n",
    "normed_images = images / max_RGB.reshape(500, 1, 1, 3)\n",
    "\n",
    "# Note: slicing max_RGB with some np.newaxis-es thrown in would also have worked (remembering, slicing and/or\n",
    "# using newaxis and/or using reshape change only the *view*, not the *data*. Like this...\n",
    "# normed_images = images / max_RGB[:, np.newaxis, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's it.  Four-line solution(after importing numpy). Did it work?  Here's a sanity check...\n",
    "\n",
    "# Are all the max-values now 1?\n",
    "print(normed_images.max(axis=(1,2)))\n",
    "\n",
    "# And more rigorously\n",
    "np.all(normed_images.max(axis=(1,2)) == 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3,48,48)-based solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Generate a collection of 500 48x48 RGB images\n",
    "images = np.random.rand(500, 3, 48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the max-intensity within each color-channel of each image...\n",
    "\n",
    "# The `axis` option tells max() to treat all the data along axes 2 & 3 (for fixed indices on axes 0 & 1)\n",
    "# as though those 48x48 values were all a single 1D chain, and to take the max among them. The net effect\n",
    "# is to replace axes 2 & 3 (again, for each fixed pair of indices on axes 0 & 1) with a scalar, namely the\n",
    "# max value.  The result will be a 500x3 array holding the maxR, maxG, and maxB values for each of the 500 images.\n",
    "max_RGB = images.max(axis=(2,3))  \n",
    "max_RGB.shape  # No need for print() -- notebook will display the last variable referenced by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to broadcast-divide all the R,G,B values on each by their respective maxes in one vectorized swoop.\n",
    "# But (500,3) isn't compatible with (500,3, 48,48) (make sure you understand *why* based on broadcasting rules).\n",
    "# But we can make them compatible by manually inserting size-1 dimensions so that the maxes take on shape\n",
    "# (500, 3, 1, 1).  And that allows us to divide in a vectorize numpythonic way.  Note: no need to have a new variable\n",
    "# point to the reshaped array, since once the division is over, we don't need to hold on to the reshaped version.\n",
    "normed_images = images / max_RGB.reshape(500, 3, 1, 1)\n",
    "\n",
    "# Note: slicing max_RGB with some np.newaxis-es thrown in would also have worked (remembering, slicing and/or\n",
    "# using newaxis and/or using reshape change only the *view*, not the *data*. Like this...\n",
    "# normed_images = images / max_RGB[:, :, np.newaxis, np.newaxis]\n",
    "# Or, since the 500 & 3 dims are grouped, we could have used an Ellipsis:\n",
    "# normed_images = images / max_RGB[..., np.newaxis, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# That's it.  Four-line solution (after importing numpy). Did it work?  Here's a sanity check...\n",
    "\n",
    "# Are all the max-values now 1?\n",
    "print(normed_images.max(axis=(2,3)))\n",
    "\n",
    "# And more rigorously\n",
    "np.all(normed_images.max(axis=(2,3)) == 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG time series (adapted from Richard Gao at UCSD)\n",
    "\n",
    "The file EEG_exp.mat contains binary MATLAB data that scipy.io.loadmat() (see below) will pull into a dictionary.  Pertinent keys in that dict:\n",
    "\n",
    "* 'EEG' -- a 1D array of over 700K floats, representing EEG data (in microV) sampled at a rate given by...\n",
    "* 'fs' -- the sampling frequency, in Hz\n",
    "* 'trial_info' -- a 2D array (300x2), each row of which contains the time (in seconds) after start of the  experiment at which some stimulus was applied to the subject followed an integer 1,2, or 3 denoting  the kind of stimulus\n",
    "\n",
    "Assume the trial_info timestamps and EEG data are both sampled ar rate fs so that no \"fudging\" is necessary with timestamps.\n",
    "\n",
    "Your goal is to do the following:\n",
    "\n",
    "1) For each event in trial_info, find the index of the corresponding time in the EEG data\n",
    "\n",
    "2) Pull out a subarray of EEG data in a window (epoch) around that time, that stretches from 0.5 seconds before the event to 1 second after the event\n",
    "\n",
    "3) Make an array of times (can be the same for all events) that labels the event time within this window as t=0, with negative/positive t values for parts of the epoch before/after the event.\n",
    "\n",
    "4) Subtract off a baseline from all the values in each epoch (baseline should be the mean of just the t<0 times in that epoch)\n",
    "\n",
    "5) Now aggregate all the events of category 1 and generate a single \"averaged\" de-baselined signal over all category 1 events.  Do likewise for the category 2 & 3 events.\n",
    "\n",
    "6) Make a single plot (with labelled axes and, ideally, a legend) showing the average signal for each of the three categories of stimulus.\n",
    "\n",
    "The key is to try to leverage what you've learned about numpy and matplotlib to do these operations efficiently and end up with fairly streamlined (but still readable) code.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io # this submodule let's us load the signal we want\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need a scipy utility function to read this MATLAB data file\n",
    "# scipy loads .mat file into a dictionary\n",
    "# the details are not crucial, we just have to unpack them into python variables.\n",
    "\n",
    "# So something like...\n",
    "EEG_data = io.loadmat('EEG_exp.mat', squeeze_me = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
