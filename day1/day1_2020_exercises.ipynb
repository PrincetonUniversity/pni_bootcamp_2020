{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 Exercises (NumPy + Matplotlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic NumPy Operations\n",
    "a) Generate an array of numbers 0-24. Reshape to a 5x5 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24]\n",
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(25)\n",
    "print(x)\n",
    "y = x.reshape(5,5)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Extract the diagonal of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  6 12 18 24]\n"
     ]
    }
   ],
   "source": [
    "z = np.diag(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Multiply the matrix by an identity matrix of the same shape. Confirm that it is identical to the original.\n",
    "\n",
    "Hint: Use `np.all` command to confirm all equal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]]\n",
      "[[ 0  1  2  3  4]\n",
      " [ 5  6  7  8  9]\n",
      " [10 11 12 13 14]\n",
      " [15 16 17 18 19]\n",
      " [20 21 22 23 24]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.identity(5)\n",
    "#print(w)\n",
    "m = y @ w \n",
    "print(m)\n",
    "print(y)\n",
    "#l = m == y\n",
    "#np.all(l)\n",
    "np.all(m==y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Join the matrix with itself and return a new matrix with shape (2,5,5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  2.  3.  4.]\n",
      "  [ 5.  6.  7.  8.  9.]\n",
      "  [10. 11. 12. 13. 14.]\n",
      "  [15. 16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23. 24.]]\n",
      "\n",
      " [[ 0.  1.  2.  3.  4.]\n",
      "  [ 5.  6.  7.  8.  9.]\n",
      "  [10. 11. 12. 13. 14.]\n",
      "  [15. 16. 17. 18. 19.]\n",
      "  [20. 21. 22. 23. 24.]]]\n"
     ]
    }
   ],
   "source": [
    "np.concatenate([m,m])\n",
    "np.ndim(np.concatenate([m,m]))\n",
    "m3d = np.concatenate([m,m]).reshape(2,5,5)\n",
    "np.ndim(m3d)\n",
    "print(m3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Compute the mean of the concatenated matrix along the first axis. Confirm its equal to the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.,  4.],\n",
       "       [ 5.,  6.,  7.,  8.,  9.],\n",
       "       [10., 11., 12., 13., 14.],\n",
       "       [15., 16., 17., 18., 19.],\n",
       "       [20., 21., 22., 23., 24.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(m3d,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Return the indices of the matrix where the elements are greater than 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3, 3, 3, 3, 4, 4, 4, 4, 4]), array([1, 2, 3, 4, 0, 1, 2, 3, 4]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m)\n",
    "np.where(m>15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Using `np.where`, set all elements of the matrix greater than 15 to 1, else 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(m>15,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Set all elements of the matrix greater than 15 to 2, less than 5 to 1, else 0.\n",
    "\n",
    "Hint: `np.where` can be passed as an input to `np.where`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  2.  3.  4.]\n",
      " [ 5.  6.  7.  8.  9.]\n",
      " [10. 11. 12. 13. 14.]\n",
      " [15. 16. 17. 18. 19.]\n",
      " [20. 21. 22. 23. 24.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 2, 2, 2, 2],\n",
       "       [2, 2, 2, 2, 2]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(m)\n",
    "np.where(m>15,2,(np.where(m<5,1,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Return the lower triangle of the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 5.  6.  0.  0.  0.]\n",
      " [10. 11. 12.  0.  0.]\n",
      " [15. 16. 17. 18.  0.]\n",
      " [20. 21. 22. 23. 24.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.tril(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Define a demean function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demean(sec):\n",
    "    mean_sec = np.apply_along_axis(mean, axis=sec, arr=m).round(2)\n",
    "    return m - mean_sec\n",
    "\n",
    "def demean2(sec):\n",
    "    mean_sec2 = np.mean(sec)\n",
    "    return sec - mean_sec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k) Apply the demean function across each row of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]\n",
      " [-2. -1.  0.  1.  2.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_sec = np.apply_along_axis(demean2, axis=1, arr=m).round(2)\n",
    "print(mean_sec)\n",
    "np.mean(mean_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## : Spike Detection\n",
    "\n",
    "In the following exercises, you will be manipulating, analyzing, and visualizing preprocessed extracellular electrophysiological data. Specifically, the following 10s recording was taken from the abdomen of a crayfish. Action potentials are readily apparent throughout the entire recording. \n",
    "\n",
    "First, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Load data.\n",
    "npz = np.load('spikes.npz')\n",
    "data = npz['data'] * 1e6      # Convert to uV\n",
    "times = npz['times']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Plot the entire raw recording. Do multiple types of spikes appear to be present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) In a recent paper, [Rey et al. (2015)](https://www.sciencedirect.com/science/article/pii/S0361923015000684) suggest a simple spike detection technique via data-driven amplitude thresholding. Specifically, they propose an automated amplitude threshold that defined as multiple of an estimate of the standard deviation of the noise:\n",
    "\n",
    "$$ \\text{threshold} = k \\cdot \\hat{\\sigma}_n $$\n",
    "\n",
    "where $k$ is a constant typically between 3-5; and $\\hat{\\sigma}_n$ is an estimate of the standard deviation of the noise, defined as:\n",
    "\n",
    "$$ \\hat{\\sigma}_n = \\frac{\\text{median} \\left( |X| \\right)}{0.6745} $$ \n",
    "\n",
    "where $|X|$ is the absolute value of the raw data.\n",
    "\n",
    "Write a function that returns the amplitude threshold as defined above. The function should accept as arguments the raw data, $X$, and the constant, $k$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Next we need a function that can detect slices of the raw signal that exceed the threshold. This ultimately becomes a clustering problem (i.e. identifying \"islands\" of signal rising above an \"ocean of noise\"). Though this is definitely doable with core NumPy, the SciPy library has built-in functions specifically written for these purposes. \n",
    "\n",
    "Because these functions are beyond the scope of the bootcamp, we have provided a peak finding function for you. The function, `peak_finder`, accepts a raw data trace and a threshold. It then finds all clusters of samples above a threshold, and returns the index and signal magnitude corresponding to the peak of each cluster.\n",
    "\n",
    "The function relies on the `measurements` tools from scipy.ndimage. For a tutorial, see [here](https://dragly.org/2013/03/25/working-with-percolation-clusters-in-python/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peak_finder(X, thresh):\n",
    "    \"\"\"Simple peak finding algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like, shape (n_times,)\n",
    "        Raw data trace.\n",
    "    thresh : float\n",
    "        Amplitude threshold.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    peak_loc : array_like, shape (n_clusters,)\n",
    "        Index of peak amplitudes.\n",
    "    peak_mag : array_like, shape (n_clusters,)\n",
    "        Magnitude of peak amplitudes.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.ndimage import measurements\n",
    "    \n",
    "    ## Error-catching.\n",
    "    assert X.ndim == 1\n",
    "    \n",
    "    ## Identify clusters.\n",
    "    clusters, ix = measurements.label(X > thresh)\n",
    "    \n",
    "    ## Identify index of peak amplitudes. \n",
    "    peak_loc = np.concatenate(measurements.maximum_position(X, labels=clusters, index=np.arange(ix)+1))\n",
    "    \n",
    "    ## Identify magnitude of peak amplitudes.\n",
    "    peak_mag = measurements.maximum(X, labels=clusters, index=np.arange(ix)+1)\n",
    "    return peak_loc, peak_mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Apply the peak detection algorithm to the raw data using a constant $k=6$. Plot a histogram of the spike amplitudes (try bins of 0-150 in increments of 5 uV). \n",
    "\n",
    "How many spikes are detected? How many types of spikes do there appear to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Plot the first second of the data. Using a scatterplot (or any other method you can think of), indicate the peak for each detected spike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Remake the plot above, but repeating the procedure with a constant $k=2$. How trustworthy is the spike detection algorithm with this more liberal threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Returning now to the detected spikes when $k=6$, define a set of boundaries that divides the spikes into three clusters. How many spikes are in each cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Action potentials last roughly 1-2 milliseconds. With this in mind, extract a 3 ms window around each detected spike; that is, extract 1.5 ms of samples on either side of the detected peak. Store each epoch according to its cluster. \n",
    "\n",
    "Hint: The data were recorded at 10 KHz meaning there are 10 samples per millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Plot each averaged spike waveform in a single plot. Add a legend denoting the spike cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RGB images (from MIT Lincoln Labs)\n",
    "\n",
    "A digital image is simply an array of numbers, which instructs a grid of pixels on a monitor to shine light of specific colors, according to the numerical values in that array.\n",
    "\n",
    "An RGB-image can thus be stored as a 3D NumPy array of shape-(V,H,3). V is the number of pixels along the vertical direction, H is the number of pixels along the horizontal, and the size-3 dimension stores the red, blue, and green color values for a given pixel. Thus a (32,32,3)\n",
    "\n",
    "array would be a 32x32 RGB image.\n",
    "\n",
    "You often work with a collection of images. Suppose we want to store N images in a single array; thus we now consider a 4D shape-(N, V, H, 3) array. For the sake of convenience, let’s simply generate a 4D-array of random numbers as a placeholder for real image data.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* generate a 4D array that holds 500, 48x48 random RGB images (think about the shape this array should have, and use np.random.rand liberally)\n",
    "\n",
    "* then, normalize those images (by dividing through by max intensity) so that the largest intensity within each color channel within each image is set to 1, but relative intensities are preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EEG time series (adapted from Richard Gao at UCSD)\n",
    "\n",
    "The file EEG_exp.mat contains binary MATLAB data that scipy.io.loadmat() (see below) will pull into a dictionary.  Pertinent keys in that dict:\n",
    "\n",
    "* 'EEG' -- a 1D array of over 700K floats, representing EEG data (in microV) sampled at a rate given by...\n",
    "* 'fs' -- the sampling frequency, in Hz\n",
    "* 'trial_info' -- a 2D array (300x2), each row of which contains the time (in seconds) after start of the  experiment at which some stimulus was applied to the subject followed an integer 1,2, or 3 denoting  the kind of stimulus\n",
    "\n",
    "Assume the trial_info timestamps and EEG data are both sampled ar rate fs so that no \"fudging\" is necessary with timestamps.\n",
    "\n",
    "Your goal is to do the following:\n",
    "\n",
    "1) For each event in trial_info, find the index of the corresponding time in the EEG data\n",
    "\n",
    "2) Pull out a subarray of EEG data in a window (epoch) around that time, that stretches from 0.5 seconds before the event to 1 second after the event\n",
    "\n",
    "3) Make an array of times (can be the same for all events) that labels the event time within this window as t=0, with negative/positive t values for parts of the epoch before/after the event.\n",
    "\n",
    "4) Subtract off a baseline from all the values in each epoch (baseline should be the mean of just the t<0 times in that epoch)\n",
    "\n",
    "5) Now aggregate all the events of category 1 and generate a single \"averaged\" de-baselined signal over all category 1 events.  Do likewise for the category 2 & 3 events.\n",
    "\n",
    "6) Make a single plot (with labelled axes and, ideally, a legend) showing the average signal for each of the three categories of stimulus.\n",
    "\n",
    "The key is to try to leverage what you've learned about numpy and matplotlib to do these operations efficiently and end up with fairly streamlined (but still readable) code.\n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import io # this submodule let's us load the signal we want\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need a scipy utility function to read this MATLAB data file\n",
    "# scipy loads .mat file into a dictionary\n",
    "# the details are not crucial, we just have to unpack them into python variables.\n",
    "\n",
    "# So something like...\n",
    "EEG_data = io.loadmat('EEG_exp.mat', squeeze_me = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
